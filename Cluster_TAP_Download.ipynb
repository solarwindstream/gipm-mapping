{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81505fb5-f949-496d-9c91-4958192e3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile \n",
    "import datetime as dt\n",
    "import cdflib\n",
    "import pandas as pd\n",
    "from requests import get # to make GET request\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from C1_Cluster_CDF_conv import C1_cdf_conv\n",
    "from C2_Cluster_CDF_conv import C2_cdf_conv\n",
    "from C3_Cluster_CDF_conv import C3_cdf_conv\n",
    "from C4_Cluster_CDF_conv import C4_cdf_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c7c352-204d-4027-aac7-28bcb45f7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define download function for calling data\n",
    "\n",
    "def download(url, params, file_name):\n",
    "    # open in binary mode\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url, params=params)\n",
    "        # write to file\n",
    "        file.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c7cb6-bc1d-42a6-b7ec-8b3f715c435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data list and download tarfiles. return list of filenames\n",
    "\n",
    "def dl_Cluster_data(cl_1, cl_2, cl_3, cl_4):\n",
    "    \n",
    "    #drop unnecessary rows (without data in!)\n",
    "    un_rows = [0,1,2,3,4]\n",
    "\n",
    "    cl_1 = cl_1.drop(un_rows)\n",
    "    cl_2 = cl_2.drop(un_rows)\n",
    "    cl_3 = cl_3.drop(un_rows)\n",
    "    cl_4 = cl_4.drop(un_rows)\n",
    "\n",
    "    #list of interval start and end points\n",
    "    cl_1_ints_start = cl_1['# SC: 1'].tolist()\n",
    "    cl_1_ints_end = cl_1[' 2'].tolist()\n",
    "    cl_2_ints_start = cl_2['# SC: 1'].tolist()\n",
    "    cl_2_ints_end = cl_2[' 2'].tolist()\n",
    "    cl_3_ints_start = cl_3['# SC: 1'].tolist()\n",
    "    cl_3_ints_end = cl_3[' 2'].tolist()\n",
    "    cl_4_ints_start = cl_4['# SC: 1'].tolist()\n",
    "    cl_4_ints_end = cl_4[' 2'].tolist()\n",
    "\n",
    "    myurl = 'https://csa.esac.esa.int/csa-sl-tap/data'\n",
    "\n",
    "    #step one: iterate over every one of the intervals, downloading the tarfiles into a folder\n",
    "    #note filenames in list for later use\n",
    "    tarfilelist_1 = []\n",
    "    tarfilelist_2 = []\n",
    "    tarfilelist_3 = []\n",
    "    tarfilelist_4 = []\n",
    "\n",
    "\n",
    "    for i, j in zip(cl_1_ints_start, cl_1_ints_end):\n",
    "        filename = 'C1tap' + i + '.tgz'\n",
    "        tarfilelist_1.append(filename)\n",
    "        query_specs = {'RETRIEVAL_TYPE': 'product',\n",
    "                   'DATASET_ID': 'C1_CP_FGM_FULL',\n",
    "                   'START_DATE': i,\n",
    "                   'END_DATE': j,\n",
    "                   'DELIVERY_FORMAT': 'CDF',\n",
    "                   'DELIVERY_INTERVAL': 'daily'}\n",
    "        download(myurl, query_specs, filename)\n",
    "\n",
    "    for i, j in zip(cl_2_ints_start, cl_2_ints_end):\n",
    "        filename = 'C2tap' + i + '.tgz'\n",
    "        tarfilelist_2.append(filename)\n",
    "        query_specs = {'RETRIEVAL_TYPE': 'product',\n",
    "                   'DATASET_ID': 'C2_CP_FGM_FULL',\n",
    "                   'START_DATE': i,\n",
    "                   'END_DATE': j,\n",
    "                   'DELIVERY_FORMAT': 'CDF',\n",
    "                   'DELIVERY_INTERVAL': 'daily'}\n",
    "        download(myurl, query_specs, filename)\n",
    "\n",
    "    for i, j in zip(cl_3_ints_start, cl_3_ints_end):\n",
    "        filename = 'C3tap' + i + '.tgz'\n",
    "        tarfilelist_3.append(filename)\n",
    "        query_specs = {'RETRIEVAL_TYPE': 'product',\n",
    "                   'DATASET_ID': 'C3_CP_FGM_FULL',\n",
    "                   'START_DATE': i,\n",
    "                   'END_DATE': j,\n",
    "                   'DELIVERY_FORMAT': 'CDF',\n",
    "                   'DELIVERY_INTERVAL': 'daily'}\n",
    "        download(myurl, query_specs, filename)\n",
    "\n",
    "    for i, j in zip(cl_4_ints_start, cl_4_ints_end):\n",
    "        filename = 'C4tap' + i + '.tgz'\n",
    "        tarfilelist_4.append(filename)\n",
    "        query_specs = {'RETRIEVAL_TYPE': 'product',\n",
    "                   'DATASET_ID': 'C4_CP_FGM_FULL',\n",
    "                   'START_DATE': i,\n",
    "                   'END_DATE': j,\n",
    "                   'DELIVERY_FORMAT': 'CDF',\n",
    "                   'DELIVERY_INTERVAL': 'daily'}\n",
    "        download(myurl, query_specs, filename)\n",
    "        \n",
    "    tarfilelist = tarfilelist_1 + tarfilelist_2 + tarfilelist_3 + tarfilelist_4\n",
    "        \n",
    "    return(tarfilelist)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eef25f55-6552-4744-ac27-a4d393dca531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tarf_extract(tarfilelist):\n",
    "\n",
    "    for i in tarfilelist:\n",
    "        with tarfile.open(i) as tar:\n",
    "            tarname = tar.getnames()\n",
    "            tar.extractall(path='/Users/apx059/Documents/23_Years_CDFs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fd96d1e-9bc3-4763-831f-0009857833bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input data list and download tarfiles. return list of filenames\n",
    "\n",
    "def dl_Cluster_data_single(int_df, sc_no, batch):\n",
    "\n",
    "    #list of interval start and end points\n",
    "    batch_start =  batch*100\n",
    "    batch_end = (batch+1)*100\n",
    "    ints_start_list = int_df['# SC: 1'].tolist()\n",
    "    ints_start = ints_start_list[batch_start:batch_end]\n",
    "    ints_end_list = int_df[' 2'].tolist()\n",
    "    ints_end = ints_end_list[batch_start:batch_end]\n",
    "    \n",
    "    myurl = 'https://csa.esac.esa.int/csa-sl-tap/data'\n",
    "\n",
    "    #step one: iterate over every one of the intervals, downloading the tarfiles into a folder\n",
    "    #note filenames in list for later use\n",
    "    tarfilelist = []\n",
    "\n",
    "    if sc_no == '1':\n",
    "        for i, j in zip(ints_start, ints_end):\n",
    "            filename = '/Users/apx059/Documents/23_Years_CDFs/Tarfiles/C1tap' + i + '.tgz'\n",
    "            tarfilelist.append(filename)\n",
    "            query_specs = {'RETRIEVAL_TYPE': 'product',\n",
    "                       'DATASET_ID': 'C1_CP_FGM_FULL',\n",
    "                       'START_DATE': i,\n",
    "                       'END_DATE': j,\n",
    "                       'DELIVERY_FORMAT': 'CDF',\n",
    "                       'DELIVERY_INTERVAL': 'daily'}\n",
    "            download(myurl, query_specs, filename)\n",
    "            \n",
    "    if sc_no == '2':\n",
    "        for i, j in zip(ints_start, ints_end):\n",
    "            filename = '/Users/apx059/Documents/23_Years_CDFs/Tarfiles/C2tap' + i + '.tgz'\n",
    "            tarfilelist.append(filename)\n",
    "            query_specs = {'RETRIEVAL_TYPE': 'product',\n",
    "                       'DATASET_ID': 'C2_CP_FGM_FULL',\n",
    "                       'START_DATE': i,\n",
    "                       'END_DATE': j,\n",
    "                       'DELIVERY_FORMAT': 'CDF',\n",
    "                       'DELIVERY_INTERVAL': 'daily'}\n",
    "            download(myurl, query_specs, filename)\n",
    "    \n",
    "    if sc_no == '3':\n",
    "        for i, j in zip(ints_start, ints_end):\n",
    "            filename = '/Users/apx059/Documents/23_Years_CDFs/Tarfiles/C3tap' + i + '.tgz'\n",
    "            tarfilelist.append(filename)\n",
    "            query_specs = {'RETRIEVAL_TYPE': 'product',\n",
    "                       'DATASET_ID': 'C3_CP_FGM_FULL',\n",
    "                       'START_DATE': i,\n",
    "                       'END_DATE': j,\n",
    "                       'DELIVERY_FORMAT': 'CDF',\n",
    "                       'DELIVERY_INTERVAL': 'daily'}\n",
    "            download(myurl, query_specs, filename)\n",
    "\n",
    "    if sc_no == '4':\n",
    "        for i, j in zip(ints_start, ints_end):\n",
    "            filename = '/Users/apx059/Documents/23_Years_CDFs/Tarfiles/C4tap' + i + '.tgz'\n",
    "            tarfilelist.append(filename)\n",
    "            query_specs = {'RETRIEVAL_TYPE': 'product',\n",
    "                       'DATASET_ID': 'C4_CP_FGM_FULL',\n",
    "                       'START_DATE': i,\n",
    "                       'END_DATE': j,\n",
    "                       'DELIVERY_FORMAT': 'CDF',\n",
    "                       'DELIVERY_INTERVAL': 'daily'}\n",
    "            download(myurl, query_specs, filename)\n",
    "\n",
    "    return(tarfilelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea303969-3736-45ae-8a78-d2a764460bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#run for 1 year, 01-02-2001 - 01-02-2002\n",
    "\n",
    "cluster_1_46 = pd.read_csv(\"/Users/apx059/Documents/Cluster Intervals 16032001-01022002/dm-intervals-c1-240731-131130.csv\")\n",
    "cluster_2_46 = pd.read_csv(\"/Users/apx059/Documents/Cluster Intervals 16032001-01022002/dm-intervals-c2-240731-131130.csv\")\n",
    "cluster_3_46 = pd.read_csv(\"/Users/apx059/Documents/Cluster Intervals 16032001-01022002/dm-intervals-c3-240731-131130.csv\")\n",
    "cluster_4_46 = pd.read_csv(\"/Users/apx059/Documents/Cluster Intervals 16032001-01022002/dm-intervals-c4-240731-131130.csv\")\n",
    "\n",
    "tf_list = dl_Cluster_data(cluster_1_46, cluster_2_46, cluster_3_46, cluster_4_46)\n",
    "\n",
    "tarf_extract(tf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97992a-d947-446b-b617-1e96f0b056b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvconv(path):\n",
    "\n",
    "    #now make those cdfs into CSVs!\n",
    "\n",
    "    #automated list of pathnames in input CDF folder\n",
    "\n",
    "    list_all = []\n",
    "    for path1 in glob.glob(path, recursive=True):\n",
    "        list_all.append(path1)\n",
    "\n",
    "    #list with only files, not folders\n",
    "    list_files = []\n",
    "\n",
    "    for element in list_all:\n",
    "        if '.cdf' in element:\n",
    "            list_files.append(element)\n",
    "            \n",
    "    #now iterate over list to convert\n",
    "\n",
    "    df_list_c1 = []\n",
    "    df_list_c2 = []\n",
    "    df_list_c3 = []\n",
    "    df_list_c4 = []\n",
    "\n",
    "\n",
    "    for i in list_files:\n",
    "        if 'C1' in i:\n",
    "            df_c1 = C1_cdf_conv(i)\n",
    "            df_list_c1.append(df_c1)\n",
    "        if 'C2' in i:\n",
    "            df_c2 = C2_cdf_conv(i)\n",
    "            df_list_c2.append(df_c2)\n",
    "        if 'C3' in i:   \n",
    "            df_c3 = C3_cdf_conv(i)\n",
    "            df_list_c3.append(df_c3)   \n",
    "        if 'C4' in i:   \n",
    "            df_c4 = C4_cdf_conv(i)\n",
    "            df_list_c4.append(df_c4)\n",
    "\n",
    "    #now save as CSVs\n",
    "\n",
    "    #first append dataframes that *aren't* empty to new list (to avoid errors)\n",
    "    df_full_c1 = []\n",
    "    df_full_c2 = []\n",
    "    df_full_c3 = []\n",
    "    df_full_c4 = []\n",
    "\n",
    "    for df in df_list_c1:\n",
    "        a = df.empty\n",
    "        if not a:\n",
    "            df_full_c1.append(df)\n",
    "\n",
    "    for df in df_list_c2:\n",
    "        a = df.empty\n",
    "        if not a:\n",
    "            df_full_c2.append(df)\n",
    "\n",
    "    for df in df_list_c3:\n",
    "        a = df.empty\n",
    "        if not a:\n",
    "            df_full_c3.append(df)\n",
    "\n",
    "    for df in df_list_c4:\n",
    "        a = df.empty\n",
    "        if not a:\n",
    "            df_full_c4.append(df) \n",
    "\n",
    "\n",
    "    #then generate a list of file names\n",
    "    df_names_1 = []\n",
    "    df_names_2 = []\n",
    "    df_names_3 = []\n",
    "    df_names_4 = []\n",
    "\n",
    "    #find first date-time value in index, stringify it and make filename\n",
    "\n",
    "    for df in df_full_c1:\n",
    "        start_time = df.index[0]\n",
    "        start = str(start_time)\n",
    "        df_names_1.append(start)\n",
    "\n",
    "\n",
    "    for df in df_full_c2:\n",
    "        start_time = df.index[0]\n",
    "        start = str(start_time)\n",
    "        df_names_2.append(start)\n",
    "\n",
    "    for df in df_full_c3:\n",
    "        start_time = df.index[0]\n",
    "        start = str(start_time)\n",
    "        df_names_3.append(start)\n",
    "\n",
    "    for df in df_full_c4:\n",
    "        start_time = df.index[0]\n",
    "        start = str(start_time)\n",
    "        df_names_4.append(start)\n",
    "        \n",
    "    for fname, df in zip(df_names_1,df_full_c1):\n",
    "        fname_full = '/Users/apx059/Documents/1 Yr Data/48 Weeks CSVs/' + fname + 'C1.csv'\n",
    "        df.to_csv(fname_full, encoding='utf-8')\n",
    "        \n",
    "    for fname, df in zip(df_names_2,df_full_c2):\n",
    "        fname_full = '/Users/apx059/Documents/1 Yr Data/48 Weeks CSVs/' + fname + 'C2.csv'\n",
    "        df.to_csv(fname_full, encoding='utf-8')\n",
    "\n",
    "    for fname, df in zip(df_names_3,df_full_c3):\n",
    "        fname_full = '/Users/apx059/Documents/1 Yr Data/48 Weeks CSVs/' + fname + 'C3.csv'\n",
    "        df.to_csv(fname_full, encoding='utf-8')\n",
    "\n",
    "    for fname, df in zip(df_names_4,df_full_c4):\n",
    "        fname_full = '/Users/apx059/Documents/1 Yr Data/48 Weeks CSVs/' + fname + 'C4.csv'\n",
    "        df.to_csv(fname_full, encoding='utf-8')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c1a08-87c2-4f85-a47e-9cebe2cf4c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#convert one csv as test\n",
    "df_c1 = C1_cdf_conv('/Users/apx059/Documents/1 Yr Data/48 Weeks CDFs/CSA_Download_20240804_1636/C1_CP_FGM_FULL/C1_CP_FGM_FULL__20010316_000000_20010316_191300_V140306.cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5896e8-c0fd-4c2e-abc6-8e2a31ecae1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvconv('/Users/apx059/Documents/1 Yr Data/48 Weeks CDFs/**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41348316-2873-4e0c-b815-04a2c453786f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2906"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c123 = pd.read_csv('/Users/apx059/Documents/Cluster Intervals-01022002-01022024/dm-intervals-c1-240902-144411.csv')\n",
    "c123.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af89fd7b-e263-4aa3-bd7a-a52ba81ef3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_1_23yr = pd.read_csv(\"/Users/apx059/Documents/Cluster Intervals-01022002-01022024/dm-intervals-c1-240902-144411.csv\")\n",
    "cluster_2_23yr = pd.read_csv(\"/Users/apx059/Documents/Cluster Intervals-01022002-01022024/dm-intervals-c2-240902-144411.csv\")\n",
    "cluster_3_23yr = pd.read_csv(\"/Users/apx059/Documents/Cluster Intervals-01022002-01022024/dm-intervals-c3-240902-144411.csv\")\n",
    "cluster_4_23yr = pd.read_csv(\"/Users/apx059/Documents/Cluster Intervals-01022002-01022024/dm-intervals-c4-240902-144411.csv\")\n",
    "\n",
    "#drop unnecessary rows (without data in!)\n",
    "un_rows = [0,1,2,3,4]\n",
    "\n",
    "cluster_1_23yr = cluster_1_23yr.drop(un_rows)\n",
    "cluster_2_23yr = cluster_2_23yr.drop(un_rows)\n",
    "cluster_3_23yr = cluster_3_23yr.drop(un_rows)\n",
    "cluster_4_23yr = cluster_4_23yr.drop(un_rows)\n",
    "\n",
    "cluster_1_23yr = cluster_1_23yr.reindex()\n",
    "cluster_2_23yr = cluster_2_23yr.reindex()\n",
    "cluster_3_23yr = cluster_3_23yr.reindex()\n",
    "cluster_4_23yr = cluster_4_23yr.reindex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a700d43-aeeb-438d-9742-8da96a6118ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#go in steps of 100\n",
    "\n",
    "batch0 = dl_Cluster_data_single(cluster_1_23yr, '1', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a510403e-d6c6-4885-bb93-6d6965ac101e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 2.51 s, total: 31.8 s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tarf_extract(batch0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "060c60d5-e6d4-4f89-9cda-e4c830c5e59d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 s, sys: 88.2 ms, total: 1.52 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#transform one cdf to csv\n",
    "\n",
    "df_c1 = C1_cdf_conv('/Users/apx059/Documents/23_Years_CDFs/CSA_Download_20240916_1227/C1_CP_FGM_FULL/C1_CP_FGM_FULL__20020201_000000_20020201_004500_V150212.cdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b10dc-1b3f-46e1-9a0b-1c2899b6005c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
